
[[aggregations-and-analysis]]
=== 집계와 분석

terms bucket 같은, 일부 집계는 ((("analysis", "aggregations and")))((("aggregations", "and analysis")))string field에서 동작한다. 
그리고 string field는 `analyzed` 또는 `not_analyzed` 이다. 분석은 집계에 얼마나 영향을 미치는가? 라는 질문이 나올 수 있다.((("strings", "analyzed or not_analyzed string fields")))((("not_analyzed fields")))((("analyzed fields")))

답은 "아주 많이"이다. 이것은 예제를 통해 가장 잘 보여줄 수 있다. 
먼저, 미국의 몇몇 주(state)를 나타내는 document를 색인 하자:

[source,js]
----
POST /agg_analysis/data/_bulk
{ "index": {}}
{ "state" : "New York" }
{ "index": {}}
{ "state" : "New Jersey" }
{ "index": {}}
{ "state" : "New Mexico" }
{ "index": {}}
{ "state" : "New York" }
{ "index": {}}
{ "state" : "New York" }
----

데이터 집합에서 유일한 주(state)와 그 수를 나타내는 목록을 만들려고 한다. 간단하게, `terms` bucket을 사용해 보자:

[source,js]
----
GET /agg_analysis/data/_search?search_type=count
{
    "aggs" : {
        "states" : {
            "terms" : {
                "field" : "state"
            }
        }
    }
}
----

다음과 같은 결과가 나올 것이다:

[source,js]
----
{
...
   "aggregations": {
      "states": {
         "buckets": [
            {
               "key": "new",
               "doc_count": 5
            },
            {
               "key": "york",
               "doc_count": 3
            },
            {
               "key": "jersey",
               "doc_count": 1
            },
            {
               "key": "mexico",
               "doc_count": 1
            }
         ]
      }
   }
}
----

이런, 우리가 원하던 것이 전혀 아니다. 집계는 주(state)의 수를 세는 것이 아니라, 개별 단어를 세고 있다. 
근본적인 이유는 간단하다. 집계는 inverted index에서 만들어지고, inverted index는 _사후 분석(post-analysis)_ 이다.

Elasticsearch에 이들 document를 추가하면, `"New York"` 이라는 문자열은 분석되고, 
token으로 만들어져, `["new", "york"]` 이 된다. 그 다음에, 이들 개별 token은 fielddata를 채우는데 사용되고, 
결과적으로 `New York` 대신 `new` 의 수를 보고 있다.

이것은 확실히 우리가 원하던 바가 아니다. 하지만, 다행히도 쉽게 수정할 수 있다.

+주(state)+ 에 대한 다중 field를 정의하고, 그것을 `not_analyzed` 로 설정해야 한다. 
이것은 `New York` 이 분석되지 않도록 한다. 즉, 그것은 집계 시에 단일 token으로 남게 된다. 
_raw_ 라는 다중 field를 지정해서, 전체 프로세스를 다시 시도해 보자.

[source,js]
----
DELETE /agg_analysis/
PUT /agg_analysis
{
  "mappings": {
    "data": {
      "properties": {
        "state" : {
          "type": "string",
          "fields": {
            "raw" : {
              "type": "string",
              "index": "not_analyzed"<1>
            }
          }
        }
      }
    }
  }
}

POST /agg_analysis/data/_bulk
{ "index": {}}
{ "state" : "New York" }
{ "index": {}}
{ "state" : "New Jersey" }
{ "index": {}}
{ "state" : "New Mexico" }
{ "index": {}}
{ "state" : "New York" }
{ "index": {}}
{ "state" : "New York" }

GET /agg_analysis/data/_search?search_type=count
{
  "aggs" : {
    "states" : {
        "terms" : {
            "field" : "state.raw" <2>
        }
    }
  }
}
----
<1> 이번에는 확실하게, +states+ field를 지정하고, `not_analyzed` 하위 field를 포함하였다.
<2> 집계는 +state+ 가 아닌 +state.raw+ 로 실행된다.

이제, 집계를 실행해 보면, 만족스러운 결과가 나온다:

[source,js]
----
{
...
   "aggregations": {
      "states": {
         "buckets": [
            {
               "key": "New York",
               "doc_count": 3
            },
            {
               "key": "New Jersey",
               "doc_count": 1
            },
            {
               "key": "New Mexico",
               "doc_count": 1
            }
         ]
      }
   }
}
----

실제로, 이런 문제는 쉽게 찾을 수 있다. 집계는 단순히 이상한 bucket을 반환하고, 
분석 문제를 제기할 것이다. 일반적이지만, 집계에 analyzed field를 사용하려는 경우가 많은 것은 아니다. 
의심이 들면, 둘 모두를 위해, 선택이 가능한 다중 field를 추가하자.((("analyzed fields", "aggregations and")))

==== 높은 cardinality의 메모리에 끼치는 영향

analyzed field의 집계를 피하려는 또 다른 이유가 있다. 높은 cardinality를 가진 field가 fielddata에 로드 되면, 
아주 많은 양의 메모리를 사용한다.((("memory usage", "high-cardinality fields")))((("cardinality", "high-cardinality fields, memory use issues"))) 
분석 프로세스는 흔히 (항상은 아니지만), 아주 많은 token과 많은 유일한 token을 생성한다. 
이것은 field의 전체 cardinality를 증가시키고, 더 많은 메모리 압박에 기여한다.((("analysis", "high-cardinality fields, memory use issues")))

분석의 특정 유형은 메모리에 대해 _매우_ 비우호적이다. ngram 분석 프로세스를 생각해 보자.((("n-grams", "memory use issues associated with"))) 
+New York+ 이라는 단어는 ngram되어, 다음과 같은 token이 된다.

- `ne`
- `ew`
- +w{nbsp}+
- +{nbsp}y+
- `yo`
- `or`
- `rk`

ngram 프로세스가 얼마나 많은 유일한 token을 생성하는지, 특히 텍스트의 단락을 분석하는 경우를 생각해 보자. 
이들을 메모리에 로드되면, 쉽게 힙(heap) 공간을 소모할 수 있다.

그래서, field에서 집계하기 전에, field가 `not_analyzed` 인지 확인하기는 시간을 가지자. 
그리고, analyzed field를 집계해야 한다면, 분석 프로세스가 터무니없는 수의 token을 생성하지 않는지 확인해야 한다.

[TIP]
==================================================

결국, field가 `analyzed` 나 `not_analyzed` 인 것은 중요하지 않다. 
field에 유일한 값이 많을수록(cardinality가 높을수록), 더 많은 메모리가 필요하다. 
모든 유일한 문자열을 메모리에 저장해야 하는 string field에서, 
이것은 특히 그렇다. 문자열이 길수록 더 많은 메모리를 사용한다.

==================================================
